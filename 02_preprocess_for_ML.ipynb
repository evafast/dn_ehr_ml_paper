{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7d6801-aed2-4f37-978a-41c8c1062786",
   "metadata": {},
   "source": [
    "**02_preprocess_for_ML**\n",
    "\n",
    "- Filtered and prepped data for Machine learning\n",
    "- main goal was feature reduction\n",
    "\n",
    "used this container: \n",
    "ml:1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3bd5ed6-1de6-41ab-a55c-b8604c0346f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<jemalloc>: MADV_DONTNEED does not work (memset will be used instead)\n",
      "<jemalloc>: (This is the expected behaviour if you are running under QEMU)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "from itertools import compress\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif, SelectPercentile, SelectFromModel\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09eb55d3-6eca-4ae5-b4d2-ff1c102db05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #this is for the outlier trimming\n",
    "    \n",
    "    class OutlierRemover(BaseEstimator,TransformerMixin):\n",
    "        def __init__(self,factor=1.5):\n",
    "            self.factor = factor\n",
    "\n",
    "        def outlier_detector(self,X,y=None):\n",
    "            X = pd.Series(X).copy()\n",
    "            q1 = X.quantile(0.1)\n",
    "            q3 = X.quantile(0.9)\n",
    "            iqr = q3 - q1\n",
    "            self.lower_bound.append(q1 - (self.factor * iqr))\n",
    "            self.upper_bound.append(q3 + (self.factor * iqr))\n",
    "\n",
    "        def fit(self,X,y=None):\n",
    "            self.lower_bound = []\n",
    "            self.upper_bound = []\n",
    "            X.apply(self.outlier_detector)\n",
    "            return self\n",
    "\n",
    "        def transform(self,X,y=None):\n",
    "            X = pd.DataFrame(X).copy()\n",
    "            for i in range(X.shape[1]):\n",
    "                x = X.iloc[:, i].copy()\n",
    "                x.loc[x < self.lower_bound[i]] = self.lower_bound[i]\n",
    "                x.loc[x > self.upper_bound[i]] = self.upper_bound[i]\n",
    "                X.iloc[:, i] = x\n",
    "            return X\n",
    "\n",
    "    outlier_remover = OutlierRemover()\n",
    "\n",
    "\n",
    "    \n",
    "    # define functions for correlation categorical - this only selects the top 15 independent features  \n",
    "    def correlation_cat(dataset, cram):\n",
    "        results_df = pd.DataFrame()  # Set of all the names of correlated columns\n",
    "        corr_matrix = cram\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                #if abs(corr_matrix.iloc[i, j]) < threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i] # getting the name of column\n",
    "                indexname = corr_matrix.index[j] # getting the name of the index\n",
    "                if (dataset[colname] == 'Not Reported').sum() <= (dataset[indexname] == 'Not Reported').sum():\n",
    "                    results_df = results_df.append(pd.DataFrame([[colname, abs(corr_matrix.iloc[i, j])]], \n",
    "                                    columns=['colname', 'coeff']))\n",
    "                else:\n",
    "                    results_df = results_df.append(pd.DataFrame([[indexname, abs(corr_matrix.iloc[i, j])]], \n",
    "                                        columns=['colname', 'coeff']))\n",
    "        # get the top 5 uncorrelated values\n",
    "        results_df = results_df.sort_values(by='coeff', ascending=True).drop_duplicates(subset=['colname'], keep='last')['colname'][:30].tolist()\n",
    "        # create list of columns to be excluded\n",
    "        exclude_list = list(set(dataset.columns.tolist()) - set(results_df))\n",
    "        return exclude_list\n",
    "    \n",
    "    # correlation numeric\n",
    "    def correlation(dataset, threshold):\n",
    "        col_corr = set()  # Set of all the names of correlated columns\n",
    "        corr_matrix = dataset.corr()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                    colname = corr_matrix.columns[i] # getting the name of column\n",
    "                    indexname = corr_matrix.index[j] # getting the name of the index\n",
    "                    if dataset[colname].isna().sum() <= dataset[indexname].isna().sum():\n",
    "                        col_corr.add(indexname)\n",
    "                    else:\n",
    "                        col_corr.add(colname)                \n",
    "        return col_corr\n",
    "\n",
    "    def cramers_v(x, y):\n",
    "        \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "            uses correction from Bergsma and Wicher,\n",
    "            Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "        \"\"\"\n",
    "        confusion_matrix = pd.crosstab(x,y)\n",
    "        chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "        n = confusion_matrix.sum().sum()\n",
    "        phi2 = chi2/n\n",
    "        r,k = confusion_matrix.shape\n",
    "        phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "        rcorr = r-((r-1)**2)/(n-1)\n",
    "        kcorr = k-((k-1)**2)/(n-1)\n",
    "        return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n",
    "    \n",
    "def preprocess_dataset(base_dir, executionid, suffix):\n",
    "    df_model = pd.read_csv(f'{base_dir}raw_data/reprocessed_model_log.csv')\n",
    "    file_name_base = str(df_model[df_model['ExecutionID'] == executionid][['aggregation_time','gap']].values[0][0]) + '_years_' + str(df_model[df_model['ExecutionID'] == executionid][['aggregation_time','gap']].values[0][1])\n",
    "    \n",
    "    # read in parquet\n",
    "    df = pd.read_parquet(f'{base_dir}raw_data/preprocessed_data/ef_agg_before_biopsy_{file_name_base}_gap.parquet', engine='pyarrow')\n",
    "\n",
    "    # drop NaN columns from parquet\n",
    "    df = df.dropna(subset=['gfb_subject_id'])\n",
    "    # convert datatype of patient_id\n",
    "    df['gfb_subject_id'] = df['gfb_subject_id'].astype(str)\n",
    "\n",
    "    # Since we get a headerless CSV file we specify the column names here.\n",
    "    feature_columns_names = {'gfb_subject_id':str,\n",
    "                             'place_of_origin':str,\n",
    "                             'disease_simple':int,\n",
    "                             'TBM_Thickening':str,\n",
    "                             'GBM_Thickening':str,\n",
    "                             'Inflammatory_Cell_Infiltrate':str,\n",
    "                             'Nodular_Sclerosis':str,\n",
    "                             'Mesangial_Hypercellularity':str,\n",
    "                             'Arteriolar_Hyalinosis':str,\n",
    "                             'source_subject_id':str,\n",
    "                             'age_biopsy': float,\n",
    "                             'diabetes_age': float,\n",
    "                             'duration_dm':str,\n",
    "                             'Diabetic_Retinopathy':str,\n",
    "                             'is_DN':str,\n",
    "                             'k_means': int,\n",
    "                             'k_means_binary':str}\n",
    "\n",
    "\n",
    "    feature_columns_names = ['gfb_subject_id',\n",
    "                             'place_of_origin',\n",
    "                             'disease_simple',\n",
    "                             'TBM_Thickening',\n",
    "                             'GBM_Thickening',\n",
    "                             'Inflammatory_Cell_Infiltrate',\n",
    "                             'Nodular_Sclerosis',\n",
    "                             'Mesangial_Hypercellularity',\n",
    "                             'Arteriolar_Hyalinosis',\n",
    "                             'source_subject_id',\n",
    "                             'age_biopsy',\n",
    "                             'diabetes_age',\n",
    "                             'duration_dm',\n",
    "                             'Diabetic_Retinopathy',\n",
    "                             'is_DN',\n",
    "                             'k_means',\n",
    "                             'k_means_binary']\n",
    "\n",
    "\n",
    "    # read file + merge with other df to filter to 523 patients + drop patient ID\n",
    "    df_target = pd.read_csv(f'{base_dir}{suffix}/ML_target_variables_trainval.csv')\n",
    "\n",
    "    df_target_temp = df_target[['gfb_subject_id', 'age_biopsy', 'diabetes_age', 'duration_dm', 'Diabetic_Retinopathy']]\n",
    "\n",
    "    df = df.merge(df_target_temp, on='gfb_subject_id', how='right')    \n",
    "    df = df.drop(columns=['gfb_subject_id', 'subject_id', 'biopsy_date', 'dob', 'freq_of_visits', 'state', 'biopsy_indication'])\n",
    "\n",
    "    #need tocoerce some columns into approriate datatype\n",
    "    df['age_biopsy'] = df['age_biopsy'].astype(float)\n",
    "    df['diabetes_age'] = df['diabetes_age'].astype(float)\n",
    "    df['ruca_code'] = df['ruca_code'].astype(str)\n",
    "    df['duration_dm'] = df['duration_dm'].fillna(value='Not Reported') # not sure why this came up\n",
    "\n",
    "    # replace any infitiny values with nans\n",
    "    df= df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    print('original', df.shape)\n",
    "\n",
    "    #drop columns that have more than 75% NaNs\n",
    "    df = df.dropna(axis='columns', thresh= round(df.shape[0]*0.25))\n",
    "\n",
    "    print('after dropna', df.shape)\n",
    "\n",
    "    # transform datatypes into either float or objects/str\n",
    "    df = pd.concat([df.select_dtypes(include='float'), \n",
    "                    df.select_dtypes(include='int64').astype(float),\n",
    "                    df.select_dtypes(include='object'), \n",
    "                    df.select_dtypes(\"bool\").astype(str)], axis=1, ignore_index=False)\n",
    "\n",
    "    #this is a dumb strategy to fill NAs in the categoriacal variables otherwise the pipeline below fails (only in Sagemaker, in the preprocessing NB it is fine), also need to force everything to str\n",
    "    mask = df.applymap(type) == float\n",
    "    d = {True: 'TRUE'}\n",
    "    df = df.where(mask, df.fillna('Not Reported'))\n",
    "    df = df.where(mask, df.replace(d))\n",
    "    df = df.where(mask, df.astype(str))\n",
    "\n",
    "    df_num = df.select_dtypes(include='float')\n",
    "    df_cat = df.select_dtypes(include= 'object')\n",
    "\n",
    "    print('number of numeric features', df_num.shape)\n",
    "    print('number of categorical features', df_cat.shape)\n",
    "\n",
    "    # drop features that are not that variable\n",
    "    loc_filter = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "\n",
    "    #correlation threshold 0.75\n",
    "    corr_threshold = 0.75\n",
    "\n",
    "    # eliminate correlated features by NaNs\n",
    "    corr_features_num = list(correlation(df_num, corr_threshold))\n",
    "    df_num = df_num.drop(corr_features_num ,axis=1)\n",
    "\n",
    "    print('number of numeric features after correlation drop', df_num.shape)\n",
    "\n",
    "    # this is to make the Cramer's V correlation matrix for the categorical features - seems overly complicated\n",
    "    factors_paired = [(i,j) for i in df_cat.columns.values for j in df_cat.columns.values] \n",
    "    cram = []\n",
    "    size_df = len(df_cat.columns.tolist())\n",
    "    for f in factors_paired:\n",
    "        if f[0] != f[1]:\n",
    "            chitest = cramers_v(df_cat[f[0]], df_cat[f[1]])   \n",
    "            cram.append(chitest)\n",
    "        else:      # for same factor pair\n",
    "            cram.append(0)\n",
    "    cram = np.array(cram).reshape((size_df,size_df)) # shape it as a matrix\n",
    "    cram = pd.DataFrame(cram, index=df_cat.columns.values, columns=df_cat.columns.values) # then a df for convenience\n",
    "\n",
    "    corr_features_cat = correlation_cat(df_cat, cram)\n",
    "    df_cat = df_cat.drop(corr_features_cat ,axis=1)\n",
    "\n",
    "    print('number of categorical features after correlation drop', df_cat.shape)\n",
    "\n",
    "    corr_features_all = corr_features_cat + corr_features_num\n",
    "    df = df.drop(corr_features_all ,axis=1)\n",
    "\n",
    "    outlier_remover = OutlierRemover()\n",
    "\n",
    "    numeric_features = df_num.columns.tolist()\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[('outlier_remover', OutlierRemover()),\n",
    "               (\"imputer\", SimpleImputer(strategy=\"median\")), # put back in because of feature selection even though RF can deal with NaNs\n",
    "               (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "               ('threshold', loc_filter),\n",
    "               ('feature_selection', SelectFromModel(RandomForestClassifier(random_state=0), max_features=15)),\n",
    "               #('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual = False), max_features=10)),\n",
    "               #('SelectPercentile', SelectPercentile(mutual_info_classif, percentile=30)),\n",
    "              ])\n",
    "\n",
    "    categorical_features = df_cat.columns.tolist()\n",
    "    categorical_transformer = Pipeline(\n",
    "            steps=[ \n",
    "                    (\"onehot\", OneHotEncoder(drop='first')),\n",
    "                    ('threshold', loc_filter),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "                (\"num\", numeric_transformer, numeric_features),\n",
    "                (\"cat\", categorical_transformer, categorical_features),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    outcome_var = 'is_DN'\n",
    "\n",
    "    y = df_target[[outcome_var]]\n",
    "    y = y.replace({'False':0, 'True':1})\n",
    "\n",
    "    X_pre = preprocess.fit_transform(df, y)\n",
    "\n",
    "    print('number of features after fit_transform', X_pre.shape)\n",
    "\n",
    "    # some ugly code which extracts the column names after filtering - how is there no off the shelf solution?!?\n",
    "    all_cat_feat = preprocess.transformers_[1][1][\"onehot\"].get_feature_names(categorical_features).tolist()\n",
    "    mask_cat = preprocess.transformers_[1][1]['threshold'].get_support().tolist()\n",
    "    filt_cat_feat = list(compress(all_cat_feat, mask_cat))\n",
    "\n",
    "    all_num_feat = preprocess.transformers_[0][2]\n",
    "    mask_num = preprocess.transformers_[0][1]['feature_selection'].get_support().tolist()\n",
    "    filt_num_feat = list(compress(all_num_feat, mask_num))\n",
    "\n",
    "    # this is to get the scaling factors\n",
    "    # add for if median is imputed\n",
    "    imp_median = SimpleImputer(strategy=\"median\")\n",
    "    imp = imp_median.fit_transform(df[filt_num_feat])\n",
    "    df_imp = pd.DataFrame(imp, columns = filt_num_feat)\n",
    "\n",
    "    # crop the outliers\n",
    "    outl = outlier_remover.fit_transform(df_imp[filt_num_feat])\n",
    "    df_outl = pd.DataFrame(outl, columns = filt_num_feat)\n",
    "\n",
    "    # get scaling factors\n",
    "    stand_df = pd.DataFrame(df_outl.mean(), columns = ['mean']).T.append(pd.DataFrame(df_outl.std(), columns = ['std']).T)\n",
    "\n",
    "    column_header = [outcome_var] + filt_num_feat + filt_cat_feat\n",
    "\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "\n",
    "    np.random.shuffle(X)\n",
    "\n",
    "    train, validation = np.split(X, [int(0.8 * len(X))])\n",
    "    \n",
    "    local_directory = f'{base_dir}{suffix}{file_name_base}/'\n",
    "    \n",
    "    path = pathlib.Path(local_directory)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pd.DataFrame(train).to_csv(f\"{base_dir}{suffix}{file_name_base}/train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(f\"{base_dir}{suffix}{file_name_base}/validation.csv\", header=False, index=False)\n",
    "    pd.DataFrame(column_header).to_csv(f\"{base_dir}{suffix}{file_name_base}/column_header.csv\", header=False, index=False) \n",
    "    pd.DataFrame(stand_df).to_csv(f\"{base_dir}{suffix}{file_name_base}/standardization.csv\", header=True, index=True)\n",
    "    \n",
    "    print(file_name_base, 'saved')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d347a73e-69de-4365-9420-8ace6ae05a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "base_dir = '/home/jovyan/work/Goldfinch/ML_paper/'\n",
    "output_suffix = 'output_files/training_output/'\n",
    "suffix = 'raw_data/intermediate_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543384b5-110c-4776-88be-eabb869b77f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original (445, 8313)\n",
      "after dropna (445, 350)\n",
      "number of numeric features (445, 155)\n",
      "number of categorical features (445, 195)\n",
      "number of numeric features after correlation drop (445, 101)\n",
      "number of categorical features after correlation drop (445, 30)\n",
      "number of features after fit_transform (445, 37)\n",
      "0_years_0 saved\n"
     ]
    }
   ],
   "source": [
    "# one round\n",
    "executionid = 1\n",
    "\n",
    "preprocess_dataset(base_dir, executionid, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95a222ea-566c-4b06-b9db-ad3c69503db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                           Version\n",
      "--------------------------------- -----------\n",
      "alembic                           1.8.1\n",
      "altair                            4.2.0\n",
      "anyio                             3.6.1\n",
      "argon2-cffi                       21.3.0\n",
      "argon2-cffi-bindings              21.2.0\n",
      "asttokens                         2.0.8\n",
      "async-generator                   1.10\n",
      "attrs                             22.1.0\n",
      "Babel                             2.10.3\n",
      "backcall                          0.2.0\n",
      "backports.functools-lru-cache     1.6.4\n",
      "beautifulsoup4                    4.11.1\n",
      "bleach                            5.0.1\n",
      "blinker                           1.4\n",
      "bokeh                             2.4.3\n",
      "Bottleneck                        1.3.5\n",
      "brotlipy                          0.7.0\n",
      "cached-property                   1.5.2\n",
      "catboost                          1.1.1\n",
      "certifi                           2022.9.24\n",
      "certipy                           0.1.3\n",
      "cffi                              1.15.1\n",
      "charset-normalizer                2.1.1\n",
      "classeval                         0.1.13\n",
      "click                             8.1.3\n",
      "cloudpickle                       2.2.0\n",
      "colorama                          0.4.5\n",
      "colourmap                         1.1.10\n",
      "conda                             4.14.0\n",
      "conda-package-handling            1.9.0\n",
      "contourpy                         1.0.5\n",
      "cryptography                      37.0.4\n",
      "cycler                            0.11.0\n",
      "Cython                            0.29.32\n",
      "cytoolz                           0.12.0\n",
      "dask                              2022.9.1\n",
      "debugpy                           1.6.3\n",
      "decorator                         5.1.1\n",
      "defusedxml                        0.7.1\n",
      "df2onehot                         1.0.1\n",
      "dill                              0.3.5.1\n",
      "distributed                       2022.9.1\n",
      "entrypoints                       0.4\n",
      "et-xmlfile                        1.1.0\n",
      "executing                         1.1.0\n",
      "fastjsonschema                    2.16.2\n",
      "flit_core                         3.7.1\n",
      "fonttools                         4.37.3\n",
      "fsspec                            2022.8.2\n",
      "funcsigs                          1.0.2\n",
      "future                            0.18.2\n",
      "gmpy2                             2.1.2\n",
      "graphviz                          0.20.1\n",
      "greenlet                          1.1.3\n",
      "h5py                              3.7.0\n",
      "HeapDict                          1.0.1\n",
      "hgboost                           1.1.3\n",
      "htmlmin                           0.1.12\n",
      "hyperopt                          0.2.7\n",
      "idna                              3.4\n",
      "imagecodecs                       2022.9.26\n",
      "ImageHash                         4.3.1\n",
      "imageio                           2.22.0\n",
      "importlib-metadata                4.11.4\n",
      "importlib-resources               5.9.0\n",
      "ipykernel                         6.16.0\n",
      "ipympl                            0.9.2\n",
      "ipython                           8.5.0\n",
      "ipython-genutils                  0.2.0\n",
      "ipywidgets                        8.0.2\n",
      "jedi                              0.18.1\n",
      "Jinja2                            3.1.2\n",
      "joblib                            1.2.0\n",
      "json5                             0.9.5\n",
      "jsonschema                        4.16.0\n",
      "jupyter-client                    7.3.4\n",
      "jupyter_core                      4.11.1\n",
      "jupyter-server                    1.19.1\n",
      "jupyter-telemetry                 0.1.0\n",
      "jupyterhub                        3.0.0\n",
      "jupyterlab                        3.4.7\n",
      "jupyterlab-pygments               0.2.2\n",
      "jupyterlab_server                 2.15.2\n",
      "jupyterlab-widgets                3.0.3\n",
      "kiwisolver                        1.4.4\n",
      "lckr-jupyterlab-variableinspector 3.0.9\n",
      "libmambapy                        0.25.0\n",
      "lightgbm                          3.3.3\n",
      "llvmlite                          0.39.1\n",
      "locket                            1.0.0\n",
      "lxml                              4.9.1\n",
      "lz4                               4.0.0\n",
      "Mako                              1.2.3\n",
      "mamba                             0.25.0\n",
      "MarkupSafe                        2.1.1\n",
      "matplotlib                        3.5.3\n",
      "matplotlib-inline                 0.1.6\n",
      "missingno                         0.5.1\n",
      "mistune                           2.0.4\n",
      "mpmath                            1.2.1\n",
      "msgpack                           1.0.4\n",
      "multimethod                       1.9\n",
      "munkres                           1.1.4\n",
      "nbclassic                         0.4.3\n",
      "nbclient                          0.6.8\n",
      "nbconvert                         7.0.0\n",
      "nbformat                          5.6.1\n",
      "nest-asyncio                      1.5.5\n",
      "networkx                          2.8.6\n",
      "notebook                          6.4.12\n",
      "notebook-shim                     0.1.0\n",
      "numba                             0.56.2\n",
      "numexpr                           2.8.3\n",
      "numpy                             1.23.3\n",
      "oauthlib                          3.2.1\n",
      "openpyxl                          3.0.10\n",
      "packaging                         21.3\n",
      "pamela                            1.0.0\n",
      "pandas                            1.5.0\n",
      "pandas-profiling                  3.4.0\n",
      "pandocfilters                     1.5.0\n",
      "parso                             0.8.3\n",
      "partd                             1.3.0\n",
      "patsy                             0.5.2\n",
      "pexpect                           4.8.0\n",
      "phik                              0.12.2\n",
      "pickleshare                       0.7.5\n",
      "Pillow                            9.2.0\n",
      "pip                               22.2.2\n",
      "pkgutil_resolve_name              1.3.10\n",
      "plotly                            5.11.0\n",
      "prometheus-client                 0.14.1\n",
      "prompt-toolkit                    3.0.31\n",
      "protobuf                          4.21.6\n",
      "psutil                            5.9.2\n",
      "ptyprocess                        0.7.0\n",
      "pure-eval                         0.2.2\n",
      "py4j                              0.10.9.7\n",
      "pyarrow                           10.0.0\n",
      "pycosat                           0.6.3\n",
      "pycparser                         2.21\n",
      "pycurl                            7.45.1\n",
      "pydantic                          1.10.2\n",
      "Pygments                          2.13.0\n",
      "PyJWT                             2.5.0\n",
      "pyOpenSSL                         22.0.0\n",
      "pyparsing                         3.0.9\n",
      "pypickle                          1.1.0\n",
      "pyrsistent                        0.18.1\n",
      "PySocks                           1.7.1\n",
      "python-dateutil                   2.8.2\n",
      "python-json-logger                2.0.1\n",
      "pytz                              2022.2.1\n",
      "PyWavelets                        1.3.0\n",
      "PyYAML                            6.0\n",
      "pyzmq                             24.0.1\n",
      "requests                          2.28.1\n",
      "ruamel.yaml                       0.17.21\n",
      "ruamel.yaml.clib                  0.2.6\n",
      "ruamel-yaml-conda                 0.15.80\n",
      "scikit-image                      0.19.3\n",
      "scikit-learn                      1.1.2\n",
      "scipy                             1.9.1\n",
      "seaborn                           0.12.0\n",
      "Send2Trash                        1.8.0\n",
      "setuptools                        65.4.0\n",
      "shap                              0.41.0\n",
      "six                               1.16.0\n",
      "sklearn                           0.0.post1\n",
      "slicer                            0.0.7\n",
      "sniffio                           1.3.0\n",
      "sortedcontainers                  2.4.0\n",
      "soupsieve                         2.3.2.post1\n",
      "SQLAlchemy                        1.4.41\n",
      "stack-data                        0.5.1\n",
      "statsmodels                       0.13.2\n",
      "sympy                             1.10.1\n",
      "tables                            3.7.0\n",
      "tangled-up-in-unicode             0.2.0\n",
      "tblib                             1.7.0\n",
      "tenacity                          8.1.0\n",
      "terminado                         0.16.0\n",
      "threadpoolctl                     3.1.0\n",
      "tifffile                          2022.8.12\n",
      "tinycss2                          1.1.1\n",
      "tomli                             2.0.1\n",
      "toolz                             0.12.0\n",
      "tornado                           6.1\n",
      "tqdm                              4.64.1\n",
      "traitlets                         5.4.0\n",
      "treeplot                          0.1.16\n",
      "typing_extensions                 4.3.0\n",
      "unicodedata2                      14.0.0\n",
      "urllib3                           1.26.11\n",
      "visions                           0.7.5\n",
      "wcwidth                           0.2.5\n",
      "webencodings                      0.5.1\n",
      "websocket-client                  1.4.1\n",
      "wget                              3.2\n",
      "wheel                             0.37.1\n",
      "widgetsnbextension                4.0.3\n",
      "xgboost                           1.7.1\n",
      "xlrd                              2.0.1\n",
      "zict                              2.2.0\n",
      "zipp                              3.8.1\n"
     ]
    }
   ],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c8022-6055-413c-8a0b-f3bd297f97bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589719e7-09eb-4f1a-bd6f-69b5edce2e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec2bb6-7c9a-4817-bdab-e4034b69652c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f57275-9838-49d8-8ee5-51f925864644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
